{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mesonet_demo_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "14ZoP1l75j5tW8fJx4kMu42FA4t2Jq0l0",
      "authorship_tag": "ABX9TyPeWFrAX9DibMFO0ZVzumIc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bf777/MesoNet/blob/master/mesonet_demo_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBeYLFh1y43a"
      },
      "source": [
        "# MesoNet\n",
        "Welcome to MesoNet, a toolbox for segmenting mesoscale calcium images! This notebook will take you through all the steps needed to process your own calcium imaging dataset using our models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ySs_sMAzwMo",
        "outputId": "c9346eb7-c241-4551-d338-e00781f1f41a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Clone MesoNet repository\n",
        "!git clone https://github.com/bf777/MesoNet.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'MesoNet' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWqA1QrmVu5U"
      },
      "source": [
        "# Prepare inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utG7t2c7JTqv",
        "outputId": "954afb1b-f919-4764-cd34-22fb84d5139d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir /content/mesonet_inputs/\n",
        "!mkdir /content/mesonet_inputs/pipeline1_2/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/mesonet_inputs/’: File exists\n",
            "mkdir: cannot create directory ‘/content/mesonet_inputs/pipeline1_2/’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFZjRHCPvUL4",
        "outputId": "299d749f-6ade-4d80-b7ae-b4722743384b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Utility to install mesonet package and associated requirements (will be replaced with pip later)\n",
        "!pip install matplotlib==3.1.3\n",
        "\n",
        "# Install DeepLabCut for pose estimation\n",
        "!pip install deeplabcut"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib==3.1.3 in /usr/local/lib/python3.7/dist-packages (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib==3.1.3) (1.15.0)\n",
            "Requirement already satisfied: deeplabcut in /usr/local/lib/python3.7/dist-packages (2.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2.5.1)\n",
            "Requirement already satisfied: statsmodels>=0.11 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.12.2)\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (4.41.1)\n",
            "Requirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2.5.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (5.5.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (4.5.3.56)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.2.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.19.5)\n",
            "Requirement already satisfied: ruamel.yaml>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.17.10)\n",
            "Requirement already satisfied: tensorpack in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.11)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.1.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.13)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.51.2)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.2.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.1.3)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.4.4)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.1.0)\n",
            "Requirement already satisfied: scikit-image>=0.17 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.18.2)\n",
            "Requirement already satisfied: filterpy in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.4.5)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->deeplabcut) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->deeplabcut) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.1->deeplabcut) (1.15.0)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.1.2 in /usr/local/lib/python3.7/dist-packages (from ruamel.yaml>=0.15.0->deeplabcut) (0.2.6)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.17->deeplabcut) (2.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.17->deeplabcut) (2021.7.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.17->deeplabcut) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deeplabcut) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deeplabcut) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deeplabcut) (1.3.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->deeplabcut) (4.4.2)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11->deeplabcut) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (2.5.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (0.12.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (0.36.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (3.17.3)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.34.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (3.7.4.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.12)\n",
            "Collecting h5py~=3.1.0\n",
            "  Using cached h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (3.3.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.1.2)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (2.5.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.12.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.0->deeplabcut) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (57.2.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (1.32.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (0.4.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (3.1.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug->deeplabcut) (1.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug->deeplabcut) (4.4.0.46)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.0->deeplabcut) (3.5.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (5.0.5)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (1.0.18)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->deeplabcut) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->deeplabcut) (0.2.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->deeplabcut) (0.34.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->deeplabcut) (0.7.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->deeplabcut) (1.0.1)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.7/dist-packages (from tables->deeplabcut) (2.7.3)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from tensorpack->deeplabcut) (0.8.9)\n",
            "Requirement already satisfied: msgpack-numpy>=0.4.4.2 in /usr/local/lib/python3.7/dist-packages (from tensorpack->deeplabcut) (0.4.7.1)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.7/dist-packages (from tensorpack->deeplabcut) (5.4.8)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from tensorpack->deeplabcut) (1.0.2)\n",
            "Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.7/dist-packages (from tensorpack->deeplabcut) (22.1.0)\n",
            "Installing collected packages: h5py\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mesonet 1.0.4.1 requires keras==2.3.1, but you have keras 2.4.3 which is incompatible.\n",
            "mesonet 1.0.4.1 requires numpy~=1.17.3, but you have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX2xRSlOG8BW"
      },
      "source": [
        "Because of how DeepLabCut operates, you now need to restart your runtime (under the Runtime menu)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgL7qiElE8O8",
        "outputId": "663ebc70-aad4-43fc-e584-104c4adba578",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# NOTE: Rerun this cell and the following two cells if you're getting an error when importing MesoNet\n",
        "%cd MesoNet/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/MesoNet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcmr-pE1yR75",
        "outputId": "16fd3a41-1bb0-4e66-8bdb-d3e1d59e0746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install MesoNet\n",
        "!python setup.py install\n",
        "\n",
        "# Reinstall OpenCV to address compatibility issue\n",
        "!pip install opencv-python==4.4.0.46\n",
        "\n",
        "# Reinstall h5py\n",
        "!pip install h5py==2.10.0\n",
        "\n",
        "# pip install mesonet"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing mesonet.egg-info/PKG-INFO\n",
            "writing dependency_links to mesonet.egg-info/dependency_links.txt\n",
            "writing requirements to mesonet.egg-info/requires.txt\n",
            "writing top-level names to mesonet.egg-info/top_level.txt\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'mesonet.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/mesonet\n",
            "copying build/lib/mesonet/model.py -> build/bdist.linux-x86_64/egg/mesonet\n",
            "copying build/lib/mesonet/data.py -> build/bdist.linux-x86_64/egg/mesonet\n",
            "copying build/lib/mesonet/utils.py -> build/bdist.linux-x86_64/egg/mesonet\n",
            "copying build/lib/mesonet/gui_train.py -> build/bdist.linux-x86_64/egg/mesonet\n",
            "copying build/lib/mesonet/gui_test.py -> build/bdist.linux-x86_64/egg/mesonet\n",
            "copying build/lib/mesonet/dlc_predict.py -> build/bdist.linux-x86_64/egg/mesonet\n",
            "copying build/lib/mesonet/__init__.py -> build/bdist.linux-x86_64/egg/mesonet\n",
            "copying build/lib/mesonet/img_augment.py -> build/bdist.linux-x86_64/egg/mesonet\n",
            "copying build/lib/mesonet/mask_functions.py -> build/bdist.linux-x86_64/egg/mesonet\n",
            "copying build/lib/mesonet/train_model.py -> build/bdist.linux-x86_64/egg/mesonet\n",
            "copying build/lib/mesonet/predict_regions.py -> build/bdist.linux-x86_64/egg/mesonet\n",
            "copying build/lib/mesonet/atlas_brain_matching.py -> build/bdist.linux-x86_64/egg/mesonet\n",
            "copying build/lib/mesonet/voxelmorph_align.py -> build/bdist.linux-x86_64/egg/mesonet\n",
            "copying build/lib/mesonet/gui_start.py -> build/bdist.linux-x86_64/egg/mesonet\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mesonet/model.py to model.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mesonet/data.py to data.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mesonet/utils.py to utils.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mesonet/gui_train.py to gui_train.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mesonet/gui_test.py to gui_test.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mesonet/dlc_predict.py to dlc_predict.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mesonet/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mesonet/img_augment.py to img_augment.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mesonet/mask_functions.py to mask_functions.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mesonet/train_model.py to train_model.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mesonet/predict_regions.py to predict_regions.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mesonet/atlas_brain_matching.py to atlas_brain_matching.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mesonet/voxelmorph_align.py to voxelmorph_align.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mesonet/gui_start.py to gui_start.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mesonet.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mesonet.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mesonet.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mesonet.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mesonet.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/mesonet-1.0.4.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing mesonet-1.0.4.1-py3.7.egg\n",
            "Removing /usr/local/lib/python3.7/dist-packages/mesonet-1.0.4.1-py3.7.egg\n",
            "Copying mesonet-1.0.4.1-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "mesonet 1.0.4.1 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/mesonet-1.0.4.1-py3.7.egg\n",
            "Processing dependencies for mesonet==1.0.4.1\n",
            "error: numpy 1.17.5 is installed but numpy~=1.19.2 is required by {'tensorflow'}\n",
            "Requirement already satisfied: opencv-python==4.4.0.46 in /usr/local/lib/python3.7/dist-packages (4.4.0.46)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==4.4.0.46) (1.19.5)\n",
            "Collecting h5py==2.10.0\n",
            "  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Installing collected packages: h5py\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.5.0 requires h5py~=3.1.0, but you have h5py 2.10.0 which is incompatible.\n",
            "mesonet 1.0.4.1 requires keras==2.3.1, but you have keras 2.4.3 which is incompatible.\n",
            "mesonet 1.0.4.1 requires numpy~=1.17.3, but you have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-2.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlkSR2Y1DMt-",
        "outputId": "d79f06af-7dd8-4ef9-e553-b600bbf8ea2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Use tensorflow 1.x (supported by DeepLabCut)\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow\n",
        "import os\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyqTdwum9zFL"
      },
      "source": [
        "We will now pull information from the OSF repository containing the DeepLabCut and U-Net models for our code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEycrQjx9yeS",
        "outputId": "08a09ae2-8053-419e-997b-ec1a69306048",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install osfclient"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting osfclient\n",
            "  Downloading osfclient-0.0.5-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from osfclient) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from osfclient) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from osfclient) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->osfclient) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->osfclient) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->osfclient) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->osfclient) (3.0.4)\n",
            "Installing collected packages: osfclient\n",
            "Successfully installed osfclient-0.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nHcGUo5-RZ6",
        "outputId": "7337b023-a813-4d11-f056-5cbe369c3f8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/\n",
        "!osf -p svztu fetch 6_Landmark_estimation_model/atlas-DongshengXiao-2020-08-03.zip mesonet_inputs/atlas-DongshengXiao-2020-08-03.zip"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "100% 643M/643M [00:07<00:00, 82.4Mbytes/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfMh09FNB8KX"
      },
      "source": [
        "!unzip -q mesonet_inputs/atlas-DongshengXiao-2020-08-03.zip -d /content/mesonet_inputs"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmNgWKOJA4Ou",
        "outputId": "d502dee0-9f4f-4a0c-a169-c24ac12f14cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/\n",
        "!osf -p svztu fetch 7_U-Net_model/DongshengXiao_brain_bundary.hdf5 MesoNet/mesonet/models/DongshengXiao_brain_bundary.hdf5"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "100% 373M/373M [00:04<00:00, 87.5Mbytes/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thn2P21Rx0LB",
        "outputId": "1bf0ea4d-a8a1-4ddc-d591-c3442959c180",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/\n",
        "!osf -p svztu fetch 7_U-Net_model/DongshengXiao_unet_motif_based_functional_atlas.hdf5 MesoNet/mesonet/models/DongshengXiao_unet_motif_based_functional_atlas.hdf5"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "100% 373M/373M [00:04<00:00, 81.9Mbytes/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rnudgt6I6FTx",
        "outputId": "3d555209-dff0-4194-b2b9-a4263021ee4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/\n",
        "!osf -p svztu fetch 8_VoxelMorph_model/VoxelMorph_Motif_based_functional_map_model_transformed1000.h5 mesonet_inputs/voxelMorph_Motif_based_functional_map_model_transformed1000.h5"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "100% 1.51M/1.51M [00:00<00:00, 143Mbytes/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oGLDOuKtKqI"
      },
      "source": [
        "# Five ways to use MesoNet\n",
        "\n",
        "MesoNet can be used through five approaches:\n",
        "1. **Atlas to brain**: Given a pre-trained DeepLabCut model that was trained to associate anatomical landmarks with corresponding points on atlases of brain regions, register an atlas of brain regions to the fixed brain imaging data using affine transformations. This approach is useful if your data has common anatomical landmarks and is the most robust to variations in image quality and orientation within your data.\n",
        "2. **Brain to atlas**: Given a pre-trained DeepLabCut model that was trained to associate anatomical landmarks with corresponding points on atlases of brain regions, the brain imaging data to a fixed atlas of brain regions using affine transformations. This approach is useful if you would like to normalize your brain images to a common template based on anatomical landmarks.\n",
        "3. **Atlas to brain + sensory maps**: Given a pre-trained DeepLabCut model that was trained to associate anatomical landmarks with corresponding points on atlases of brain regions as well as a set of folders containing functional brain activity for that animal that is consistent across animals, register an atlas of brain regions to the fixed brain imaging data using affine transformations. This approach is useful if you have consistent peaks of functional activity across animals that you would like to use in the alignment processes.\n",
        "4. **Motif-based functional maps (MBFMs) + U-Net**: Given a pre-trained U-Net model that was trained to associate brain imaging data with atlases of brain regions, predict the locations of brain regions in the data without the use of landmarks. The brain imaging data should be motif-based functional maps (MBFMs) calculated using the associated MATLAB code (using seqNMF). This approach is useful if one wishes to mark functional regions based on more complex features of the data (e.g. a motif-based functional map) than landmarks.\n",
        "5. **Motif-based functional maps (MBFMs) + Brain-to-atlas + VoxelMorph**: Given a pre-trained VoxelMorph model that was trained to compute a non-linear transformation between a template functional brain atlas and brain image data, predict the locations of brain regions in the data. In particular, this approach can register each input brain image to a user-defined template functional atlas. The brain imaging data should be motif-based functional maps (MBFMs) calculated using the associated MATLAB code (using seqNMF). This approach is useful if your images are consistently oriented and you want to compare the predicted locations of brain regions across different images.\n",
        "\n",
        "We will now copy over some sample input images for each of these five pipelines from OSF to our inputs folder. If, instead, you would like to upload your own images create a folder inside `mesonet_inputs` and put your images inside that folder; then, skip the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpXToLmIKx3o",
        "outputId": "03c9dd01-51c0-4e9d-c263-10c615a86b77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Run this cell to fetch sample data from OSF\n",
        "%cd /content/\n",
        "!osf -p fy6e3 clone /content/mesonet_inputs/example_data\n",
        "\n",
        "# # Pipeline 1 + 2\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline1_2/0.png mesonet_inputs/pipeline1_2/0.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline1_2/1.png mesonet_inputs/pipeline1_2/1.png\n",
        "\n",
        "# # Pipeline 3\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline3_sensory/sensory_maps/1/tail.png mesonet_inputs/pipeline3_sensory/sensory_maps/1/tail.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline3_sensory/sensory_maps/1/visual.png mesonet_inputs/pipeline3_sensory/sensory_maps/1/visual.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline3_sensory/sensory_maps/1/whisker.png mesonet_inputs/pipeline3_sensory/sensory_maps/1/whisker.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline3_sensory/sensory_maps/2/tail.png mesonet_inputs/pipeline3_sensory/sensory_maps/2/tail.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline3_sensory/sensory_maps/2/visual.png mesonet_inputs/pipeline3_sensory/sensory_maps/2/visual.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline3_sensory/sensory_maps/2/whisker.png mesonet_inputs/pipeline3_sensory/sensory_maps/2/whisker.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline3_sensory/sensory_maps/3/tail.png mesonet_inputs/pipeline3_sensory/sensory_maps/3/tail.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline3_sensory/sensory_maps/3/visual.png mesonet_inputs/pipeline3_sensory/sensory_maps/3/visual.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline3_sensory/sensory_maps/3/whisker.png mesonet_inputs/pipeline3_sensory/sensory_maps/3/whisker.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline3_sensory/sensory_raw/1.png mesonet_inputs/pipeline3_sensory/sensory_raw/1.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline3_sensory/sensory_raw/2.png mesonet_inputs/pipeline3_sensory/sensory_raw/2.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline3_sensory/sensory_raw/3.png mesonet_inputs/pipeline3_sensory/sensory_raw/3.png\n",
        "\n",
        "# # Pipeline 4\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline4_MBFM-U-Net/0.png mesonet_inputs/pipeline4_MBFM-U-Net/0.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline4_MBFM-U-Net/1.png mesonet_inputs/pipeline4_MBFM-U-Net/1.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline4_MBFM-U-Net/2.png mesonet_inputs/pipeline4_MBFM-U-Net/2.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline4_MBFM-U-Net/3.png mesonet_inputs/pipeline4_MBFM-U-Net/3.png\n",
        "\n",
        "# # Pipeline 5\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline5_VoxelMorph/1.png mesonet_inputs/pipeline5_VoxelMorph/1.png\n",
        "# !osf -p svztu fetch 0_Example_data/Automated_pipeline_sample_data/pipeline5_VoxelMorph/2.png mesonet_inputs/pipeline5_VoxelMorph/2.png"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "0files [00:00, ?files/s]\n",
            "100% 15.2k/15.2k [00:00<00:00, 51.4Mbytes/s]\n",
            "1files [00:06,  6.07s/files]\n",
            "100% 15.6k/15.6k [00:00<00:00, 52.5Mbytes/s]\n",
            "2files [00:07,  4.61s/files]\n",
            "100% 21.0k/21.0k [00:00<00:00, 46.1Mbytes/s]\n",
            "3files [00:09,  3.91s/files]\n",
            "100% 21.3k/21.3k [00:00<00:00, 50.7Mbytes/s]\n",
            "4files [00:10,  3.11s/files]\n",
            "100% 19.6k/19.6k [00:00<00:00, 49.7Mbytes/s]\n",
            "5files [00:12,  2.54s/files]\n",
            "100% 19.5k/19.5k [00:00<00:00, 56.1Mbytes/s]\n",
            "6files [00:13,  2.14s/files]\n",
            "100% 13.8k/13.8k [00:00<00:00, 49.0Mbytes/s]\n",
            "7files [00:17,  2.71s/files]\n",
            "100% 13.8k/13.8k [00:00<00:00, 46.8Mbytes/s]\n",
            "8files [00:18,  2.28s/files]\n",
            "100% 13.4k/13.4k [00:00<00:00, 41.7Mbytes/s]\n",
            "9files [00:19,  1.96s/files]\n",
            "100% 157k/157k [00:00<00:00, 122Mbytes/s]\n",
            "10files [00:23,  2.40s/files]\n",
            "100% 146k/146k [00:00<00:00, 127Mbytes/s]\n",
            "11files [00:24,  2.07s/files]\n",
            "100% 167k/167k [00:00<00:00, 114Mbytes/s]\n",
            "12files [00:26,  1.91s/files]\n",
            "100% 135k/135k [00:00<00:00, 101Mbytes/s]\n",
            "13files [00:28,  1.99s/files]\n",
            "100% 120k/120k [00:00<00:00, 110Mbytes/s]\n",
            "14files [00:30,  2.07s/files]\n",
            "100% 126k/126k [00:00<00:00, 120Mbytes/s]\n",
            "15files [00:32,  1.96s/files]\n",
            "100% 128k/128k [00:00<00:00, 117Mbytes/s]\n",
            "16files [00:34,  2.10s/files]\n",
            "100% 127k/127k [00:00<00:00, 104Mbytes/s]\n",
            "17files [00:36,  1.91s/files]\n",
            "100% 137k/137k [00:00<00:00, 98.4Mbytes/s]\n",
            "18files [00:37,  1.74s/files]\n",
            "100% 7.04k/7.04k [00:00<00:00, 13.4Mbytes/s]\n",
            "19files [00:39,  1.88s/files]\n",
            "100% 6.88k/6.88k [00:00<00:00, 23.3Mbytes/s]\n",
            "20files [00:40,  2.04s/files]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hActhRFLDl9o"
      },
      "source": [
        "Now, input the information about your input and output images, as well as the U-Net and DeepLabCut models that you would like to use. The default values will use the test data that we've included in the MesoNet git repository (in `MesoNet/mesonet/tests/test_input`). If you're using your own input data, replace `input_filename` below with the name of a folder in `mesonet_inputs` containing your input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXx8XqLH1wSb",
        "cellView": "form"
      },
      "source": [
        "#@title Input information for the model\n",
        "input_file_name = 'pipeline1_2'  #@param {type: \"string\"}\n",
        "input_file_sensory_raw_name = 'sensory_raw'  #@param {type: \"string\"}\n",
        "input_file_sensory_maps_name = 'sensory_maps'  #@param {type: \"string\"}\n",
        "input_file_MBFM_name = 'pipeline4_MBFM-U-Net'  #@param {type: \"string\"}\n",
        "input_file_voxelmorph_name = 'pipeline5_VoxelMorph'  #@param {type: \"string\"}\n",
        "\n",
        "output_file_atlas_brain_name = 'mesonet_outputs_atlas_brain'  #@param {type: \"string\"}\n",
        "output_file_brain_atlas_name = 'mesonet_outputs_brain_atlas'  #@param {type: \"string\"}\n",
        "output_file_sensory_name = 'mesonet_outputs_sensory'  #@param {type: \"string\"}\n",
        "output_file_MBFM_U_Net_name = 'mesonet_outputs_MBFM_U_Net'  #@param {type: \"string\"}\n",
        "output_file_voxelmorph_name = 'mesonet_outputs_voxelmorph'  #@param {type: \"string\"}\n",
        "\n",
        "model_name = 'DongshengXiao_brain_bundary.hdf5' #@param {type: \"string\"}\n",
        "u_net_only_model_name = 'DongshengXiao_unet_motif_based_functional_atlas.hdf5'\n",
        "dlc_model_name = 'atlas-DongshengXiao-2020-08-03'  #@param {type: \"string\"}\n",
        "voxelmorph_model_name = 'voxelMorph_Motif_based_functional_map_model_transformed1000.h5'  #@param {type: \"string\"}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO5rzOEE20vz"
      },
      "source": [
        "# Set up filepaths based on your inputs\n",
        "input_path_root = os.path.join('/content','mesonet_inputs', 'example_data')\n",
        "input_file = os.path.join(input_path_root, 'osfstorage', 'Automated_pipeline_sample_data', input_file_name)\n",
        "input_file_sensory_raw = os.path.join(input_path_root, \n",
        "                                      'osfstorage', 'Automated_pipeline_sample_data', 'pipeline3_sensory', input_file_sensory_raw_name)\n",
        "input_file_sensory_maps = os.path.join(input_path_root, \n",
        "                                       'osfstorage', 'Automated_pipeline_sample_data', 'pipeline3_sensory', input_file_sensory_maps_name)\n",
        "input_file_MBFM = os.path.join(input_path_root, \n",
        "                               'osfstorage', 'Automated_pipeline_sample_data', input_file_MBFM_name)\n",
        "input_file_voxelmorph = os.path.join(input_path_root, \n",
        "                                     'osfstorage', 'Automated_pipeline_sample_data', input_file_voxelmorph_name)\n",
        "\n",
        "output_file_atlas_brain = os.path.join('/content','mesonet_outputs', output_file_atlas_brain_name)\n",
        "output_file_brain_atlas = os.path.join('/content','mesonet_outputs', output_file_brain_atlas_name)\n",
        "output_file_sensory = os.path.join('/content','mesonet_outputs', output_file_sensory_name)\n",
        "output_file_MBFM_U_Net = os.path.join('/content','mesonet_outputs', output_file_MBFM_U_Net_name)\n",
        "output_file_voxelmorph = os.path.join('/content','mesonet_outputs', output_file_voxelmorph_name)\n",
        "\n",
        "model = os.path.join('/content', 'MesoNet', 'mesonet', 'models', model_name)\n",
        "u_net_only_model = os.path.join('/content', 'MesoNet', 'mesonet', 'models', u_net_only_model_name)\n",
        "voxelmorph_model = os.path.join('/content','mesonet_inputs', voxelmorph_model_name)\n",
        "dlc_config = os.path.join('/content','mesonet_inputs', dlc_model_name, 'config.yaml')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hv6LzKSC_v8",
        "outputId": "afca6d62-8860-41ab-b6d0-07b12e09da58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir '/content/mesonet_outputs'\n",
        "!mkdir '/content/mesonet_outputs/mesonet_outputs_atlas_brain'\n",
        "!mkdir '/content/mesonet_outputs/mesonet_outputs_brain_atlas'\n",
        "!mkdir '/content/mesonet_outputs/mesonet_outputs_sensory'\n",
        "!mkdir '/content/mesonet_outputs/mesonet_outputs_MBFM_U_Net'\n",
        "!mkdir '/content/mesonet_outputs/mesonet_outputs_voxelmorph'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/mesonet_outputs’: File exists\n",
            "mkdir: cannot create directory ‘/content/mesonet_outputs/mesonet_outputs_atlas_brain’: File exists\n",
            "mkdir: cannot create directory ‘/content/mesonet_outputs/mesonet_outputs_brain_atlas’: File exists\n",
            "mkdir: cannot create directory ‘/content/mesonet_outputs/mesonet_outputs_sensory’: File exists\n",
            "mkdir: cannot create directory ‘/content/mesonet_outputs/mesonet_outputs_MBFM_U_Net’: File exists\n",
            "mkdir: cannot create directory ‘/content/mesonet_outputs/mesonet_outputs_voxelmorph’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLul3lSb3WmY"
      },
      "source": [
        "Now that we've told Colab where to find the input and output folders, let's define the configuration file!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM1CKImF0OSo"
      },
      "source": [
        "# Configure MesoNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQiL1hPwhDrh"
      },
      "source": [
        "NOTE: If you get the error `ModuleNotFoundError: No module named 'mesonet'`, rerun the cell near the top of the notebook that starts with `%cd MesoNet/`, as well as the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_x_r_X2SLXS"
      },
      "source": [
        "# Set this environment variable to help MesoNet find the git repo location\n",
        "os.environ[\"MESONET_GIT\"]='/content/MesoNet/mesonet/'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tbS1rlK3Osl",
        "outputId": "53b40679-a1ee-462d-cc67-433e901652ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We need to make sure that DeepLabCut doesn't run with a GUI (which isn't\n",
        "# supported in Colab).\n",
        "os.environ[\"DLClight\"]=\"True\"\n",
        "\n",
        "# Import mesonet and define the configuration file for each pipeline\n",
        "import mesonet\n",
        "## 1. Atlas to brain\n",
        "# Atlas-to-brain warp with U-Net and DeepLabCut\n",
        "print('1. Atlas-to-brain warp with U-Net and DeepLabCut')\n",
        "config_file_atlas_brain = mesonet.config_project(input_file, output_file_atlas_brain, 'test', \n",
        "                                                 atlas_to_brain_align=True, use_voxelmorph=False, \n",
        "                                                 use_unet=True, use_dlc=True, sensory_match=False, \n",
        "                                                 mat_save=False, olfactory_check=True,\n",
        "                                                 config=dlc_config, model=model)\n",
        "\n",
        "## 2. Brain to atlas\n",
        "# Brain-to-atlas warp with DeepLabCut\n",
        "print('2. Brain-to-atlas warp with DeepLabCut')\n",
        "config_file_brain_atlas = mesonet.config_project(input_file, output_file_brain_atlas, 'test', \n",
        "                                                 atlas_to_brain_align=False, use_voxelmorph=False, \n",
        "                                                 use_unet=True, use_dlc=True, sensory_match=False, \n",
        "                                                 mat_save=False, olfactory_check=True, \n",
        "                                                 config=dlc_config, model=model)\n",
        "\n",
        "## 3. Atlas to brain + sensory\n",
        "# Atlas-to-brain warp with U-Net, DeepLabCut, and sensory maps\n",
        "print('3. Atlas-to-brain warp with U-Net, DeepLabCut, and sensory maps')\n",
        "config_file_sensory = mesonet.config_project(input_file_sensory_raw, output_file_sensory, 'test',\n",
        "                                             atlas_to_brain_align=True, use_voxelmorph=False, \n",
        "                                             use_unet=True, use_dlc=True, sensory_match=True, \n",
        "                                             sensory_path=input_file_sensory_maps, mat_save=False, \n",
        "                                             config=dlc_config, model=model)\n",
        "\n",
        "## 4. MBFM + U-Net\n",
        "# Motif-based functional maps (MBFMs) with atlas directly applied using U-Net\n",
        "print('4. Motif-based functional maps (MBFMs) with atlas directly applied using U-Net')\n",
        "config_file_MBFM_U_Net = mesonet.config_project(input_file_MBFM, output_file_MBFM_U_Net, 'test', \n",
        "                                                atlas_to_brain_align=True, use_voxelmorph=False, \n",
        "                                                use_unet=True, use_dlc=False, sensory_match=False, \n",
        "                                                mat_save=False, mask_generate=False, \n",
        "                                                config=dlc_config, model=u_net_only_model)\n",
        "\n",
        "## 5. VoxelMorph\n",
        "# Local deformation warp with VoxelMorph and DeepLabCut\n",
        "print('5. Local deformation warp with VoxelMorph and DeepLabCut')\n",
        "config_file_voxelmorph = mesonet.config_project(input_file_voxelmorph, output_file_voxelmorph, 'test', \n",
        "                                                atlas_to_brain_align=False, use_voxelmorph=True, \n",
        "                                                use_unet=True, use_dlc=True, sensory_match=False, mat_save=False, \n",
        "                                                config=dlc_config, model=model, \n",
        "                                                align_once=True, olfactory_check=True, \n",
        "                                                voxelmorph_model=voxelmorph_model)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Atlas-to-brain warp with U-Net and DeepLabCut\n",
            "/content/MesoNet/mesonet/\n",
            "2. Brain-to-atlas warp with DeepLabCut\n",
            "/content/MesoNet/mesonet/\n",
            "3. Atlas-to-brain warp with U-Net, DeepLabCut, and sensory maps\n",
            "/content/MesoNet/mesonet/\n",
            "4. Motif-based functional maps (MBFMs) with atlas directly applied using U-Net\n",
            "/content/MesoNet/mesonet/\n",
            "5. Local deformation warp with VoxelMorph and DeepLabCut\n",
            "/content/MesoNet/mesonet/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wmSornuIcWr"
      },
      "source": [
        "The config file (by default in each of the output folders) contains information about how MesoNet will run for each pipeline. We'll be using these config files as an input to the last two functions needed to run MesoNet. \n",
        "\n",
        "Now, we will run each of the five pipelines in turn:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb16iWZm0T9_"
      },
      "source": [
        "# Run MesoNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6xYOJczyIUB"
      },
      "source": [
        "## Pipeline 1: Atlas-to-brain\n",
        "Firstly, we will identify the outer edges of the cortex:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uR1JWHR0Xa8"
      },
      "source": [
        "### Predict regions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nLe62A-843I",
        "outputId": "4787bc53-66e8-48ba-a471-1288b8d088eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/\n",
        "mesonet.predict_regions(config_file_atlas_brain)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/MesoNet/mesonet/models/DongshengXiao_brain_bundary.hdf5\n",
            "/content/mesonet_inputs/example_data/osfstorage/Automated_pipeline_sample_data/pipeline1_2\n",
            "2/2 [==============================] - 11s 5s/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FhQX6a189Zc"
      },
      "source": [
        "Next, we will identify and use cortical landmarks to align the atlas to the brain:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdkZHKo70bKX"
      },
      "source": [
        "### Predict landmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg1NLh3E9QTY",
        "outputId": "a731dee0-7674-42d1-a5ed-a736250731fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mesonet.predict_dlc(config_file_atlas_brain)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "/content/mesonet_inputs/example_data/osfstorage/Automated_pipeline_sample_data/pipeline1_2/0.png\n",
            "/content/mesonet_inputs/example_data/osfstorage/Automated_pipeline_sample_data/pipeline1_2/1.png\n",
            "Using snapshot-1030000 for model /content/mesonet_inputs/atlas-DongshengXiao-2020-08-03/dlc-models/iteration-2/atlasAug3-trainset95shuffle1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Analyzing all the videos in the directory...\n",
            "Starting to analyze %  /content/mesonet_outputs/mesonet_outputs_atlas_brain/dlc_output/tmp_video.mp4\n",
            "/content/mesonet_outputs/mesonet_outputs_atlas_brain/dlc_output  already exists!\n",
            "Loading  /content/mesonet_outputs/mesonet_outputs_atlas_brain/dlc_output/tmp_video.mp4\n",
            "Duration of video [s]:  0.07 , recorded with  30.0 fps!\n",
            "Overall # of frames:  2  found with (before cropping) frame dimensions:  512 512\n",
            "Starting to extract posture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r10it [00:02,  4.76it/s]              \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving results in /content/mesonet_outputs/mesonet_outputs_atlas_brain/dlc_output...\n",
            "Saving csv poses!\n",
            "The videos are analyzed. Now your research can truly start! \n",
            " You can create labeled videos with 'create_labeled_video'\n",
            "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
            "/content/mesonet_outputs/mesonet_outputs_atlas_brain/dlc_output  already exists!\n",
            "Starting to process video: /content/mesonet_outputs/mesonet_outputs_atlas_brain/dlc_output/tmp_video.mp4\n",
            "Loading /content/mesonet_outputs/mesonet_outputs_atlas_brain/dlc_output/tmp_video.mp4 and data.\n",
            "No filtered data file found in /content/mesonet_outputs/mesonet_outputs_atlas_brain/dlc_output for video tmp_video and scorer DLC_resnet50_atlasAug3shuffle1_1030000.\n",
            "Landmark prediction complete!\n",
            "Performing first transformation of atlas 0...\n",
            "Performing second transformation of atlas 0...\n",
            "Performing first transformation of atlas 1...\n",
            "Performing second transformation of atlas 1...\n",
            "LEN CNTS: 43\n",
            "LEN LABELS: 43\n",
            "Mask 0 saved!\n",
            "LEN CNTS: 42\n",
            "LEN LABELS: 42\n",
            "Mask 1 saved!\n",
            "Analysis complete! Check the outputs in the folders of /content/mesonet_outputs/mesonet_outputs_atlas_brain/dlc_output/../output_overlay.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7lCpR53H3EO"
      },
      "source": [
        "Congratulations, you're all done with this first pipeline! You can now check the outputs in the `mesonet_output_atlas_brain` folder. The segmented brain data can be found in `mesonet_output_atlas_brain/output_overlay`.\n",
        "\n",
        "The following four pipelines will follow a similar pattern:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8Bbzh6wzcCU"
      },
      "source": [
        "## Pipeline 2: Brain-to-atlas\n",
        "This time, we will directly identify and use cortical landmarks to align the brain to the atlas:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT0hngL331wT"
      },
      "source": [
        "### Predict regions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gAq9qv733vc",
        "outputId": "07ebc43d-5492-4f18-f4dc-1d7cf453050f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/\n",
        "mesonet.predict_regions(config_file_brain_atlas)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/MesoNet/mesonet/models/DongshengXiao_brain_bundary.hdf5\n",
            "/content/mesonet_inputs/example_data/osfstorage/Automated_pipeline_sample_data/pipeline1_2\n",
            "2/2 [==============================] - 11s 5s/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E778cvgzngw"
      },
      "source": [
        "### Predict landmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2jjquarzp34",
        "outputId": "34df065d-3642-4eb2-e593-6980a9c27e11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/\n",
        "mesonet.predict_dlc(config_file_brain_atlas)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "2\n",
            "/content/mesonet_inputs/example_data/osfstorage/Automated_pipeline_sample_data/pipeline1_2/0.png\n",
            "/content/mesonet_inputs/example_data/osfstorage/Automated_pipeline_sample_data/pipeline1_2/1.png\n",
            "Using snapshot-1030000 for model /content/mesonet_inputs/atlas-DongshengXiao-2020-08-03/dlc-models/iteration-2/atlasAug3-trainset95shuffle1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Analyzing all the videos in the directory...\n",
            "Starting to analyze %  /content/mesonet_outputs/mesonet_outputs_brain_atlas/dlc_output/tmp_video.mp4\n",
            "/content/mesonet_outputs/mesonet_outputs_brain_atlas/dlc_output  already exists!\n",
            "Loading  /content/mesonet_outputs/mesonet_outputs_brain_atlas/dlc_output/tmp_video.mp4\n",
            "Duration of video [s]:  0.07 , recorded with  30.0 fps!\n",
            "Overall # of frames:  2  found with (before cropping) frame dimensions:  512 512\n",
            "Starting to extract posture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r10it [00:02,  4.91it/s]              "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving results in /content/mesonet_outputs/mesonet_outputs_brain_atlas/dlc_output...\n",
            "Saving csv poses!\n",
            "The videos are analyzed. Now your research can truly start! \n",
            " You can create labeled videos with 'create_labeled_video'\n",
            "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/mesonet_outputs/mesonet_outputs_brain_atlas/dlc_output  already exists!\n",
            "Starting to process video: /content/mesonet_outputs/mesonet_outputs_brain_atlas/dlc_output/tmp_video.mp4\n",
            "Loading /content/mesonet_outputs/mesonet_outputs_brain_atlas/dlc_output/tmp_video.mp4 and data.\n",
            "No filtered data file found in /content/mesonet_outputs/mesonet_outputs_brain_atlas/dlc_output for video tmp_video and scorer DLC_resnet50_atlasAug3shuffle1_1030000.\n",
            "Landmark prediction complete!\n",
            "Performing first transformation of atlas 0...\n",
            "Performing second transformation of atlas 0...\n",
            "Performing first transformation of atlas 1...\n",
            "Performing second transformation of atlas 1...\n",
            "LEN CNTS: 44\n",
            "LEN LABELS: 44\n",
            "Mask 0 saved!\n",
            "LEN CNTS: 44\n",
            "LEN LABELS: 44\n",
            "Mask 1 saved!\n",
            "Analysis complete! Check the outputs in the folders of /content/mesonet_outputs/mesonet_outputs_brain_atlas/dlc_output/../output_overlay.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74FqIOkDz-Qx"
      },
      "source": [
        "You can now check the outputs in the `mesonet_output_brain_atlas` folder. The segmented brain data can be found in `mesonet_output_brain_atlas/output_overlay`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkXNc2Ap0Gaw"
      },
      "source": [
        "## Pipeline 3: Atlas-to-Brain + sensory\n",
        "Now, we return to the Brain-to-Atlas approach while also using peaks of functional activity that are common across animals as a further alignment step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxYqk3Qy1K-w"
      },
      "source": [
        "### Predict regions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgz-lh-w1N8m",
        "outputId": "07310217-24eb-433f-d494-b947c67be625",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/\n",
        "mesonet.predict_regions(config_file_sensory)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/MesoNet/mesonet/models/DongshengXiao_brain_bundary.hdf5\n",
            "/content/mesonet_inputs/example_data/osfstorage/Automated_pipeline_sample_data/pipeline3_sensory/sensory_raw\n",
            "3/3 [==============================] - 16s 5s/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_9tcIor1PH1"
      },
      "source": [
        "### Predict landmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZC7PQ0c1Q2O",
        "outputId": "704696ff-9a0f-4219-c1fb-43aba77e17a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mesonet.predict_dlc(config_file_sensory)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "/content/mesonet_inputs/example_data/osfstorage/Automated_pipeline_sample_data/pipeline3_sensory/sensory_raw/1.png\n",
            "/content/mesonet_inputs/example_data/osfstorage/Automated_pipeline_sample_data/pipeline3_sensory/sensory_raw/2.png\n",
            "/content/mesonet_inputs/example_data/osfstorage/Automated_pipeline_sample_data/pipeline3_sensory/sensory_raw/3.png\n",
            "Using snapshot-1030000 for model /content/mesonet_inputs/atlas-DongshengXiao-2020-08-03/dlc-models/iteration-2/atlasAug3-trainset95shuffle1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Analyzing all the videos in the directory...\n",
            "Starting to analyze %  /content/mesonet_outputs/mesonet_outputs_sensory/dlc_output/tmp_video.mp4\n",
            "/content/mesonet_outputs/mesonet_outputs_sensory/dlc_output  already exists!\n",
            "Loading  /content/mesonet_outputs/mesonet_outputs_sensory/dlc_output/tmp_video.mp4\n",
            "Duration of video [s]:  0.1 , recorded with  30.0 fps!\n",
            "Overall # of frames:  3  found with (before cropping) frame dimensions:  512 512\n",
            "Starting to extract posture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r10it [00:02,  3.57it/s]              "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving results in /content/mesonet_outputs/mesonet_outputs_sensory/dlc_output...\n",
            "Saving csv poses!\n",
            "The videos are analyzed. Now your research can truly start! \n",
            " You can create labeled videos with 'create_labeled_video'\n",
            "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/mesonet_outputs/mesonet_outputs_sensory/dlc_output  already exists!\n",
            "Starting to process video: /content/mesonet_outputs/mesonet_outputs_sensory/dlc_output/tmp_video.mp4\n",
            "Loading /content/mesonet_outputs/mesonet_outputs_sensory/dlc_output/tmp_video.mp4 and data.\n",
            "No filtered data file found in /content/mesonet_outputs/mesonet_outputs_sensory/dlc_output for video tmp_video and scorer DLC_resnet50_atlasAug3shuffle1_1030000.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Landmark prediction complete!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Performing first transformation of atlas 0...\n",
            "Performing second transformation of atlas 0...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Performing first transformation of atlas 1...\n",
            "Performing second transformation of atlas 1...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Performing first transformation of atlas 2...\n",
            "Performing second transformation of atlas 2...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LEN CNTS: 49\n",
            "LEN LABELS: 49\n",
            "WARNING: label 0 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 25 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 27 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 31 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 32 was not found in region. Order of labels may be incorrect!\n",
            "Mask 0 saved!\n",
            "LEN CNTS: 46\n",
            "LEN LABELS: 46\n",
            "WARNING: label 22 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 24 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 26 was not found in region. Order of labels may be incorrect!\n",
            "Mask 1 saved!\n",
            "LEN CNTS: 44\n",
            "LEN LABELS: 44\n",
            "Mask 2 saved!\n",
            "Analysis complete! Check the outputs in the folders of /content/mesonet_outputs/mesonet_outputs_sensory/dlc_output/../output_overlay.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqdDUZHi1ecN"
      },
      "source": [
        "You can now check the outputs in the `mesonet_output_sensory` folder. The segmented brain data can be found in `mesonet_output_sensory/output_overlay`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTFM4mFg19eE"
      },
      "source": [
        "## Pipeline 4: MBFM + U-Net\n",
        "Our input for this pipeline will be a set of motif-based functional maps (MBFMs) - brain images that summarize patterns of spatio-temporal activity that are common across animals. You can generate these using a MATLAB script running [seqNMF](https://github.com/FeeLab/seqNMF) - such a script is available in `4_Data_code/New_end_to_end_code` on our [OSF repository](https://osf.io/svztu/). We will then use a U-Net model to directly segment the brain image into functional regions - no need for atlas registration here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ogr73HsT29_W"
      },
      "source": [
        "### Predict regions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmKekkzW3A4o",
        "outputId": "b51f928d-0bbc-4cb8-abf3-2c4b55487855",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/\n",
        "mesonet.predict_regions(config_file_MBFM_U_Net)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/MesoNet/mesonet/models/DongshengXiao_unet_motif_based_functional_atlas.hdf5\n",
            "/content/mesonet_inputs/example_data/osfstorage/Automated_pipeline_sample_data/pipeline4_MBFM-U-Net\n",
            "4/4 [==============================] - 20s 5s/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LEN CNTS: 44\n",
            "LEN LABELS: 44\n",
            "Mask 0 saved!\n",
            "LEN CNTS: 54\n",
            "LEN LABELS: 54\n",
            "WARNING: label 1 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 11 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 24 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 45 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 46 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 47 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 48 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 49 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 51 was not found in region. Order of labels may be incorrect!\n",
            "Mask 1 saved!\n",
            "LEN CNTS: 52\n",
            "LEN LABELS: 52\n",
            "WARNING: label 9 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 24 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 27 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 29 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 44 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 45 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 47 was not found in region. Order of labels may be incorrect!\n",
            "Mask 2 saved!\n",
            "LEN CNTS: 42\n",
            "LEN LABELS: 42\n",
            "Mask 3 saved!\n",
            "Analysis complete! Check the outputs in the folders of /content/mesonet_outputs/mesonet_outputs_MBFM_U_Net/output_overlay.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FL5w9Zn3IPC"
      },
      "source": [
        "You can now check the outputs in the `mesonet_output_MBFM_U_Net` folder. The segmented brain data can be found in `mesonet_output_MBFM_U_Net/output_overlay`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wn10Qar3WxY"
      },
      "source": [
        "## Pipeline 5: VoxelMorph\n",
        "Our input for this pipeline will be a raw brain image followed by an MBFM (see Pipeline 4 description for details). We will use VoxelMorph - a local deformation technique - to register the MBFM to an atlas based on a template image in the MesoNet repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8yk7XCg0InQ"
      },
      "source": [
        "### Predict regions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7F3yQSe0OjA",
        "outputId": "c4c8eaf4-523b-411d-99bd-7409e048234e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/\n",
        "mesonet.predict_regions(config_file_voxelmorph)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/MesoNet/mesonet/models/DongshengXiao_brain_bundary.hdf5\n",
            "/content/mesonet_inputs/example_data/osfstorage/Automated_pipeline_sample_data/pipeline5_VoxelMorph\n",
            "2/2 [==============================] - 10s 5s/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlR21DdB3yoC"
      },
      "source": [
        "### Predict landmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA75tW_u336F",
        "outputId": "bb7375c7-c281-4348-afb6-31b7530e31d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mesonet.predict_dlc(config_file_voxelmorph)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "/content/mesonet_inputs/example_data/osfstorage/Automated_pipeline_sample_data/pipeline5_VoxelMorph/0.png\n",
            "/content/mesonet_inputs/example_data/osfstorage/Automated_pipeline_sample_data/pipeline5_VoxelMorph/1.png\n",
            "Using snapshot-1030000 for model /content/mesonet_inputs/atlas-DongshengXiao-2020-08-03/dlc-models/iteration-2/atlasAug3-trainset95shuffle1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Analyzing all the videos in the directory...\n",
            "Starting to analyze %  /content/mesonet_outputs/mesonet_outputs_voxelmorph/dlc_output/tmp_video.mp4\n",
            "/content/mesonet_outputs/mesonet_outputs_voxelmorph/dlc_output  already exists!\n",
            "Loading  /content/mesonet_outputs/mesonet_outputs_voxelmorph/dlc_output/tmp_video.mp4\n",
            "Duration of video [s]:  0.07 , recorded with  30.0 fps!\n",
            "Overall # of frames:  2  found with (before cropping) frame dimensions:  512 512\n",
            "Starting to extract posture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r10it [00:02,  4.91it/s]              "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving results in /content/mesonet_outputs/mesonet_outputs_voxelmorph/dlc_output...\n",
            "Saving csv poses!\n",
            "The videos are analyzed. Now your research can truly start! \n",
            " You can create labeled videos with 'create_labeled_video'\n",
            "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/mesonet_outputs/mesonet_outputs_voxelmorph/dlc_output  already exists!\n",
            "Starting to process video: /content/mesonet_outputs/mesonet_outputs_voxelmorph/dlc_output/tmp_video.mp4\n",
            "Loading /content/mesonet_outputs/mesonet_outputs_voxelmorph/dlc_output/tmp_video.mp4 and data.\n",
            "No filtered data file found in /content/mesonet_outputs/mesonet_outputs_voxelmorph/dlc_output for video tmp_video and scorer DLC_resnet50_atlasAug3shuffle1_1030000.\n",
            "Landmark prediction complete!\n",
            "Performing first transformation of atlas 0...\n",
            "Performing second transformation of atlas 0...\n",
            "Performing first transformation of atlas 1...\n",
            "Performing second transformation of atlas 1...\n",
            "WARNING: Entity <bound method SpatialTransformer.call of <voxelmorph.tf.layers.SpatialTransformer object at 0x7f321dfb93d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Results saved!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/MesoNet/mesonet/voxelmorph_align.py:101: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
            "  x_data = rgb2gray(x_data)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Entity <bound method SpatialTransformer.call of <voxelmorph.tf.layers.SpatialTransformer object at 0x7f321b90f410>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "LEN CNTS: 47\n",
            "LEN LABELS: 47\n",
            "WARNING: label 0 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 1 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 2 was not found in region. Order of labels may be incorrect!\n",
            "Mask 0 saved!\n",
            "LEN CNTS: 47\n",
            "LEN LABELS: 47\n",
            "WARNING: label 0 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 1 was not found in region. Order of labels may be incorrect!\n",
            "WARNING: label 2 was not found in region. Order of labels may be incorrect!\n",
            "Mask 1 saved!\n",
            "Analysis complete! Check the outputs in the folders of /content/mesonet_outputs/mesonet_outputs_voxelmorph/dlc_output/../output_overlay.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7j9mAAV4AC-"
      },
      "source": [
        "You can now check the outputs in the `mesonet_output_voxelmorph` folder. The segmented brain data can be found in `mesonet_output_voxelmorph/output_overlay`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjHOCd9f5DgV"
      },
      "source": [
        "# Conclusion\n",
        "These five pipelines can be directly accessed in the graphical user interface (GUI) that is available for MesoNet for ease of use. Furthermore, you can customize your pipeline by changing the options defined in `mesonet.config_project` for the CLI and in the GUI - you can use our [Quick Start Guide](https://github.com/bf777/MesoNet/wiki/Quick-Start-Guide) and [Config File Guide](https://github.com/bf777/MesoNet/wiki/Config-File-Guide) for guidance."
      ]
    }
  ]
}